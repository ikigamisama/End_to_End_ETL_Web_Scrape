{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156acc9a",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'get'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 239\u001b[0m\n\u001b[0;32m    233\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m    238\u001b[0m e \u001b[38;5;241m=\u001b[39m ProductsETL()\n\u001b[1;32m--> 239\u001b[0m \u001b[43me\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_links\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://pcx.com.ph/collections/smartphones\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;66;03m# product_list = [\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m#  'https://www.myphone.com.ph/product/myx12/',\u001b[39;00m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m#  'https://www.myphone.com.ph/product/mywx2-pro/'\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;66;03m# q = asyncio.run(e.extract_scrape_content(product_list[1], '#page-content'))\u001b[39;00m\n\u001b[0;32m    247\u001b[0m \u001b[38;5;66;03m# e.transform(q, product_list[1])\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 229\u001b[0m, in \u001b[0;36mProductsETL.extract_links\u001b[1;34m(self, url)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_links\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m    228\u001b[0m     soup \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_scrape_content(url, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#MainContent\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m--> 229\u001b[0m     urls \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhttps://pcx.com.ph\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhref\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mt4s_box_pr_grid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdiv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    231\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: urls})\n\u001b[0;32m    232\u001b[0m     df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPcExpress\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 229\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_links\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame:\n\u001b[0;32m    228\u001b[0m     soup \u001b[38;5;241m=\u001b[39m asyncio\u001b[38;5;241m.\u001b[39mrun(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_scrape_content(url, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m#MainContent\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m--> 229\u001b[0m     urls \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pcx.com.ph\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[43mproduct\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ma\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhref\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m product \u001b[38;5;129;01min\u001b[39;00m soup\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m, class_\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mt4s_box_pr_grid\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m'\u001b[39m)]\n\u001b[0;32m    231\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124murl\u001b[39m\u001b[38;5;124m\"\u001b[39m: urls})\n\u001b[0;32m    232\u001b[0m     df\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshop\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPcExpress\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'get'"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import nest_asyncio\n",
    "import random\n",
    "import requests\n",
    "import time\n",
    "import re\n",
    "import math\n",
    "import json\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "from fake_useragent import UserAgent\n",
    "from tenacity import (\n",
    "    retry,\n",
    "    retry_if_exception_type,\n",
    "    stop_after_attempt,\n",
    "    wait_random,\n",
    ")\n",
    "nest_asyncio.apply()\n",
    "\n",
    "headers = {\n",
    "    \"Accept\": 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Encoding': 'gzip, deflate, br, zstd',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "    \"User-Agent\": UserAgent().random,\n",
    "    'Priority': \"u=0, i\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "    \"Cookie\": \"wp_ga4_customerGroup=NOT+LOGGED+IN; private_content_version=1fd9b0bb9111f815fb7cc0a2e1b795ad; aws-waf-token=b91f6c13-c3c0-4ec3-a7d8-550794a8bab3:BgoAk3cBOREiAAAA:PKYZTtk3ZLHvTjqbebPH7ufj4dmmpWy1IlXw54gtVwoFfEp98V/nK037tyC5DSvl9OzpfidRLN+1piQTIY2t/NTfkI0XZyGaSMZ/3npm2dZE+AjvhGX6qTmw1wqTX5LRfU22N36ziK2KEU9pZAHu9DJzmdRP1i7Crd1RGecYNV/y3r+7tDwE0A2HqpfwOIMBWFw=\",\n",
    "    \"referer\": 'https://www.google.com/',\n",
    "    \"Sec-Ch-Ua\": \"\\\"Not(A:Brand\\\";v=\\\"99\\\", \\\"Opera GX\\\";v=\\\"118\\\", \\\"Chromium\\\";v=\\\"133\\\"\",\n",
    "    \"Sec-Ch-Ua-Mobile\": \"?0\",\n",
    "    \"Sec-Ch-Ua-Platform\": \"\\\"Windows\\\"\",\n",
    "    \"Sec-Fetch-Dest\": \"document\",\n",
    "    \"Sec-Fetch-Mode\": \"navigate\",\n",
    "    \"Sec-Fetch-Site\": \"same-origin\",\n",
    "    \"Sec-Fetch-User\": \"?1\"\n",
    "}\n",
    "\n",
    "MAX_RETRIES = 10\n",
    "MAX_WAIT_BETWEEN_REQ = 2\n",
    "MIN_WAIT_BETWEEN_REQ = 0\n",
    "REQUEST_TIMEOUT = 30\n",
    "\n",
    "class ProductsETL():\n",
    "    def __init__(self):\n",
    "        self.session = requests.Session()\n",
    "        \n",
    "    @retry(\n",
    "        wait=wait_random(min=MIN_WAIT_BETWEEN_REQ, max=MAX_WAIT_BETWEEN_REQ),\n",
    "        stop=stop_after_attempt(MAX_RETRIES),\n",
    "        retry=retry_if_exception_type(requests.RequestException),\n",
    "        reraise=True,\n",
    "    )\n",
    "    async def extract_scrape_content(self, url, selector):\n",
    "        soup = None\n",
    "        browser = None\n",
    "        try:\n",
    "            async with async_playwright() as p:\n",
    "                browser_args = {\n",
    "                    \"headless\": True,\n",
    "                    \"args\": [\"--disable-blink-features=AutomationControlled\"]\n",
    "                }\n",
    "\n",
    "                browser = await p.chromium.launch(**browser_args)\n",
    "                context = await browser.new_context(\n",
    "                    locale=\"en-US\",\n",
    "                    user_agent=UserAgent().random,\n",
    "                    viewport={\"width\": 1280, \"height\": 800},\n",
    "                    device_scale_factor=1,\n",
    "                    is_mobile=False,\n",
    "                    has_touch=False,\n",
    "                    screen={\"width\": 1280, \"height\": 800},\n",
    "                    timezone_id=\"Asia/Manila\"\n",
    "                )\n",
    "\n",
    "                page = await context.new_page()\n",
    "\n",
    "                await page.add_init_script(\"Object.defineProperty(navigator, 'webdriver', {get: () => undefined})\")\n",
    "                await page.set_extra_http_headers(headers)\n",
    "                await page.goto(url, wait_until=\"domcontentloaded\")\n",
    "                await page.wait_for_selector(selector, timeout=30000)\n",
    "\n",
    "                for _ in range(random.randint(3, 6)):\n",
    "                    await page.mouse.wheel(0, random.randint(300, 700))\n",
    "                    await asyncio.sleep(random.uniform(0.5, 1))\n",
    "\n",
    "                for _ in range(random.randint(5, 10)):\n",
    "                    await page.mouse.move(random.randint(0, 800), random.randint(0, 600))\n",
    "                    await asyncio.sleep(random.uniform(0.5, 1))\n",
    "\n",
    "                rendered_html = await page.content()\n",
    "                return BeautifulSoup(rendered_html, \"html.parser\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "        finally:\n",
    "            if browser:\n",
    "                await browser.close()\n",
    "\n",
    "    async def _scroll_products(self, url):\n",
    "        soup = None\n",
    "        browser = None\n",
    "        try:\n",
    "            async with async_playwright() as p:\n",
    "                browser_args = {\n",
    "                    \"headless\": True,\n",
    "                    \"args\": [\"--disable-blink-features=AutomationControlled\"]\n",
    "                }\n",
    "\n",
    "                browser = await p.chromium.launch(**browser_args)\n",
    "                context = await browser.new_context(\n",
    "                    user_agent=UserAgent().random,\n",
    "                    viewport={\"width\": random.randint(\n",
    "                        1200, 1600), \"height\": random.randint(800, 1200)},\n",
    "                    locale=\"en-US\"\n",
    "                )\n",
    "\n",
    "                page = await context.new_page()\n",
    "                await page.set_extra_http_headers(headers)\n",
    "\n",
    "                await page.goto(url, wait_until=\"domcontentloaded\")\n",
    "                await page.wait_for_selector('#root-product-list', timeout=30000)\n",
    "\n",
    "                print(\n",
    "                    \"Starting to scrape the product list (Infinite scroll scrape)...\")\n",
    "\n",
    "                scroll_step = 1500\n",
    "                scroll_delay = 5\n",
    "\n",
    "                previous_count = 0\n",
    "                same_count_retries = 0\n",
    "                max_retries = 3\n",
    "\n",
    "                while True:\n",
    "                    # Scroll to the bottom\n",
    "                    await page.evaluate(f'window.scrollBy(0, {scroll_step})')\n",
    "                    await asyncio.sleep(scroll_delay)\n",
    "\n",
    "                    # Check if the spinner exists\n",
    "                    current_count = await page.evaluate(\"\"\"\n",
    "                        () => document.querySelectorAll('div.item-siminia-product-grid-item-3do').length\n",
    "                    \"\"\")\n",
    "\n",
    "                    print(f\"Current item count: {current_count}\")\n",
    "\n",
    "                    if current_count > previous_count:\n",
    "                        previous_count = current_count\n",
    "                        scroll_step += scroll_step\n",
    "                        same_count_retries = 0\n",
    "                    else:\n",
    "                        same_count_retries += 1\n",
    "                        print(f\"No new items loaded. Retry {same_count_retries}/{max_retries}\")\n",
    "\n",
    "                        if same_count_retries >= max_retries:\n",
    "                            print(\"No more items being loaded. Done scrolling.\")\n",
    "                            break\n",
    "\n",
    "                print(\"Scraping complete. Extracting content...\")\n",
    "\n",
    "                rendered_html = await page.content()\n",
    "                print(\n",
    "                    f\"Successfully extracted data from {url}\"\n",
    "                )\n",
    "                soup = BeautifulSoup(rendered_html, \"html.parser\")\n",
    "                return soup.find_all('div', class_=\"item-siminia-product-grid-item-3do\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred: {e}\")\n",
    "\n",
    "        finally:\n",
    "            if browser:\n",
    "                await browser.close()\n",
    "\n",
    "    def extract_from_url(self, method: str, url: str, params: dict = None, data: dict = None, headers: dict = None, verify: bool = True) -> BeautifulSoup:\n",
    "        try:\n",
    "            # Parse request response\n",
    "            response = self.session.request(\n",
    "                method=method, url=url, params=params, data=data, headers=headers, verify=verify)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "            print(\n",
    "                f\"Successfully extracted data from {url} {response.status_code}\"\n",
    "            )\n",
    "            sleep_time = random.uniform(\n",
    "                MIN_WAIT_BETWEEN_REQ, MAX_WAIT_BETWEEN_REQ)\n",
    "            print(f\"Sleeping for {sleep_time} seconds...\")\n",
    "            return soup\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error in parsing {url}: {e}\")\n",
    "\n",
    "\n",
    "    def transform(self, soup, url):\n",
    "        product_shop = \"MyPhone\"\n",
    "        product_name = soup.find(\n",
    "            'meta', attrs={'property': 'og:title'}).get('content')\n",
    "        product_brand = \"MyPhone\"\n",
    "        product_rating = '0/5'\n",
    "\n",
    "        product_description = soup.find(\n",
    "            'meta', attrs={'property': 'og:description'}).get('content')\n",
    "        product_url = url\n",
    "        product_variant = None\n",
    "        product_image_url = soup.find(\n",
    "            'meta', attrs={'itemprop': 'image'}).get('content')\n",
    "        \n",
    "        \n",
    "\n",
    "        feature_data = {\n",
    "            'height': None,\n",
    "            'width': None,\n",
    "            'length': None,\n",
    "            'gross_weight': None,\n",
    "            'net_weight': None,\n",
    "            'screen_size': None,\n",
    "            'sim_slot': None,\n",
    "            'processor': None,\n",
    "            'memory': None,\n",
    "            'camera': None,\n",
    "            'battery': None\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "    def extract_links(self, url: str) -> pd.DataFrame:\n",
    "        soup = asyncio.run(self.extract_scrape_content(url, '#MainContent'))\n",
    "        urls = ['https://pcx.com.ph' + product.find('a').get('href') for product in soup.find('div', class_=\"t4s_box_pr_grid\").find_all('div', class_=\"t4s-product\")]\n",
    "       \n",
    "        df = pd.DataFrame({\"url\": urls})\n",
    "        df.insert(0, \"shop\", \"PcExpress\")\n",
    "        return df\n",
    "    \n",
    "        \n",
    "\n",
    "\n",
    "e = ProductsETL()\n",
    "e.extract_links('https://pcx.com.ph/collections/smartphones')\n",
    "\n",
    "# product_list = [\n",
    "#  'https://www.myphone.com.ph/product/myx12/',\n",
    "#  'https://www.myphone.com.ph/product/mywx2-pro/'\n",
    "# ]\n",
    "\n",
    "# q = asyncio.run(e.extract_scrape_content(product_list[1], '#page-content'))\n",
    "# e.transform(q, product_list[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
